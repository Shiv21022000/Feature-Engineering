{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S4DWgCu5l9BZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is a parameter?"
      ],
      "metadata": {
        "id": "LvrFbVBVl-Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, a parameter is a variable that the model learns from the training data to make predictions.\n",
        "\n",
        "üìå Key Points:\n",
        "Parameters define the model: They are the internal configuration that gets updated during training.\n",
        "\n",
        "Learned automatically: Parameters are adjusted through optimization (e.g., gradient descent) to minimize error or loss.\n",
        "\n",
        "They are not set manually; the algorithm finds their optimal values from the data."
      ],
      "metadata": {
        "id": "oqbWhllImGpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is correlation?\n",
        "#What does negative correlation mean?"
      ],
      "metadata": {
        "id": "MNvlW4HQmMEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "It is typically measured using the correlation coefficient (often Pearson‚Äôs r), which ranges from ‚Äì1 to +1.\n",
        "\n",
        "üìà Values of Correlation Coefficient (r):\n",
        "r value\tInterpretation\n",
        "+1\tPerfect positive correlation\n",
        "0\tNo correlation\n",
        "‚Äì1\tPerfect negative correlation\n",
        "\n",
        "üìâ What is Negative Correlation?\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "Example: As exercise time increases, body weight may decrease.\n",
        "\n",
        "So, they move in opposite directions.\n",
        "\n",
        "üîÅ In numbers:\n",
        "If\n",
        "ùëü\n",
        "=\n",
        "‚àí\n",
        "0.8\n",
        "r=‚àí0.8, that suggests a strong negative correlation.\n",
        "\n",
        "If\n",
        "ùëü\n",
        "=\n",
        "‚àí\n",
        "0.2\n",
        "r=‚àí0.2, that suggests a weak negative correlation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ_2GvnqmYA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "8UPk2-cRmb8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that enables systems to automatically learn and improve from experience (data) without being explicitly programmed.\n",
        "\n",
        "It focuses on building algorithms that can identify patterns, make decisions, or predict outcomes based on data.\n",
        "\n",
        "üß± Main Components of Machine Learning:\n",
        "Component\tDescription\n",
        "1. Data\tThe foundation of ML; includes input features and often expected outputs (labels).\n",
        "2. Model\tThe mathematical structure or algorithm that makes predictions or decisions. Examples: linear regression, decision trees, neural networks.\n",
        "3. Features\tThe input variables used to make predictions. Also called predictors or independent variables.\n",
        "4. Labels\tThe target output or result you're trying to predict (only in supervised learning).\n",
        "5. Training\tThe process of feeding data to the model so it can learn patterns and relationships.\n",
        "6. Algorithm\tThe procedure used to adjust the model based on the data (e.g., gradient descent, decision tree splits).\n",
        "7. Evaluation\tAssessing the model‚Äôs performance using metrics like accuracy, precision, recall, RMSE, etc.\n",
        "8. Inference/Prediction\tUsing the trained model to make predictions on new, unseen data.\n",
        "9. Hyperparameters\tSettings chosen before training that affect model learning (e.g., learning rate, number of layers)."
      ],
      "metadata": {
        "id": "3B6kXBafmjWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "EYom6fSJmmzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, the loss value is a numerical representation of how well (or poorly) the model is performing. It measures the difference between the predicted output and the actual target (ground truth).\n",
        "\n",
        "üìâ What Is Loss?\n",
        "A low loss means the model's predictions are close to the actual values.\n",
        "\n",
        "A high loss means the predictions are far from the actual values.\n",
        "\n",
        "The loss function guides the learning process‚Äîby minimizing this value, the model gets better.\n",
        "\n",
        "üìå Why Is Loss Important?\n",
        "Reason\tExplanation\n",
        "Performance Indicator\tIt tells how well the model is doing on training and validation data.\n",
        "Model Comparison\tHelps compare different models or algorithms‚Äîlower loss = better model.\n",
        "Optimization Guide\tLoss is minimized during training using algorithms like gradient descent.\n",
        "Detect Overfitting\tLarge gap between training and validation loss indicates overfitting.\n"
      ],
      "metadata": {
        "id": "NkHPAeUEmpdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "5ibyxfosmvEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In statistics and machine learning, variables are the features or attributes used to describe data. They are broadly classified into two types: continuous and categorical.\n",
        "\n",
        "1. üìà Continuous Variables\n",
        "These are numerical variables that can take any value within a range (including decimals).\n",
        "\n",
        "‚úî Characteristics:\n",
        "Infinite possible values within a range.\n",
        "\n",
        "Arise from measurement (e.g., height, weight, temperature).\n",
        "\n",
        "Can be ordered and arithmetically manipulated.\n",
        "\n",
        "üß† Examples:\n",
        "Height in cm: 162.5 cm, 170.2 cm\n",
        "\n",
        "Temperature in ¬∞C: 36.6, 37.0\n",
        "\n",
        "Salary: ‚Çπ45,000, ‚Çπ58,550.25\n",
        "\n",
        "2. üßÆ Categorical Variables\n",
        "These are variables that take discrete values representing categories or groups.\n",
        "\n",
        "‚úî Characteristics:\n",
        "Values represent labels or names.\n",
        "\n",
        "Can be nominal (no order) or ordinal (ordered).\n",
        "\n",
        "Cannot be meaningfully averaged.\n",
        "\n",
        "üß† Examples:\n",
        "Type\tExample\n",
        "Nominal\tGender (Male, Female), Color (Red, Blue)\n",
        "Ordinal\tEducation Level (High School, College, Masters)"
      ],
      "metadata": {
        "id": "c9T1L_KdmzBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "2MJknFbBm4p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning algorithms can't work directly with categorical (non-numeric) data, so we must convert categorical variables into numerical format before training the model.\n",
        "\n",
        "üìå Common Techniques to Handle Categorical Variables:\n",
        "1. Label Encoding\n",
        "Converts each category into a unique integer.\n",
        "\n",
        "Suitable for ordinal data (where order matters).\n",
        "\n",
        "üîç Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "Color: [Red, Green, Blue] ‚Üí [0, 1, 2]\n",
        "‚ö† Caution:\n",
        "Not good for nominal data because it introduces an artificial order (0 < 1 < 2).\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Suitable for nominal data (no natural order).\n",
        "\n",
        "üîç Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "Color: [Red, Green, Blue]\n",
        "‚Üí Red: [1, 0, 0], Green: [0, 1, 0], Blue: [0, 0, 1]\n",
        "üì¶ Tools:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "3. Ordinal Encoding\n",
        "Assigns ordered integers to ordinal variables.\n",
        "\n",
        "üîç Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "Size: [Small, Medium, Large] ‚Üí [0, 1, 2]\n",
        "üü¢ Useful when categories have meaningful ranking.\n",
        "4. Binary Encoding / Hashing\n",
        "Reduces dimensionality for high-cardinality features.\n",
        "\n",
        "Converts categories to binary and splits them into separate columns.\n",
        "\n",
        "üìå Use Case:\n",
        "Useful when a column has hundreds or thousands of unique categories (e.g., ZIP codes, product IDs).\n",
        "\n",
        "5. Target / Mean Encoding\n",
        "Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "‚ö† Risk:\n",
        "Can cause data leakage if not used with proper cross-validation."
      ],
      "metadata": {
        "id": "UFQ-wdK_nDEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8tBUJbD4m7Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "M7t7BzkfnDxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, we split the dataset into two (or more) parts to build and evaluate the model effectively:\n",
        "\n",
        "1. üèãÔ∏è‚Äç‚ôÇÔ∏è Training Dataset\n",
        "This is the portion of the data used to train the model.\n",
        "\n",
        "The model learns patterns and relationships from this data.\n",
        "\n",
        "During training, the algorithm adjusts internal parameters to minimize loss or error.\n",
        "\n",
        "üìå Think of it as:\n",
        "‚ÄúTeaching the model using known input and output.‚Äù\n",
        "\n",
        "2. üß™ Testing Dataset\n",
        "This is a separate portion of the data used to evaluate the model's performance.\n",
        "\n",
        "It has not been seen by the model during training.\n",
        "\n",
        "Helps measure how well the model generalizes to new, unseen data.\n",
        "\n",
        "üìå Think of it as:\n",
        "‚ÄúExamining how well the model performs on unknown data.‚Äù\n",
        "\n",
        "üß† Why Split the Data?\n",
        "Because using the same data for training and testing can lead to overfitting, where the model performs well on known data but fails on new data.\n",
        "\n",
        "üìä Common Split Ratios:\n",
        "Training Set\tTesting Set\n",
        "80%\t20%\n",
        "70%\t30%\n",
        "75%\t25%\n",
        "\n",
        "Sometimes a third set is used:\n",
        "\n",
        "3. üîÅ Validation Set (for model tuning)\n",
        "Used during training to fine-tune hyperparameters.\n",
        "\n",
        "Prevents overfitting before final testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "PX3pnr3enKlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "klANyKX1nLYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in Scikit-learn (a popular Python machine learning library) that provides a collection of functions and classes to prepare your data before training machine learning models.\n",
        "\n",
        "üì¶ It helps you transform raw input data into a format suitable for modeling, such as scaling, normalizing, encoding, and imputing.\n",
        "\n",
        "üîß Common Tools in sklearn.preprocessing:\n",
        "Class / Function\tPurpose\n",
        "StandardScaler\tStandardizes features by removing the mean and scaling to unit variance.\n",
        "MinMaxScaler\tScales features to a fixed range (usually 0 to 1).\n",
        "RobustScaler\tScales using the median and IQR (robust to outliers).\n",
        "Normalizer\tNormalizes each row to have unit norm (used for text or image data).\n",
        "LabelEncoder\tConverts categorical labels (target values) into numeric format.\n",
        "OneHotEncoder\tConverts categorical features into one-hot binary vectors.\n",
        "OrdinalEncoder\tEncodes categorical features as integers (useful for ordinal features).\n",
        "Binarizer\tConverts numerical values into 0/1 based on a threshold.\n",
        "PolynomialFeatures\tGenerates interaction and power terms for features (feature engineering).\n",
        "FunctionTransformer\tApply any custom function to transform data."
      ],
      "metadata": {
        "id": "cAtAnV0knOGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. What is a Test set?"
      ],
      "metadata": {
        "id": "oGcaRkP0nTvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a portion of your dataset that is set aside to evaluate the final performance of your trained machine learning model.\n",
        "\n",
        "üîç Purpose:\n",
        "The test set is used to simulate how the model will perform on real, unseen data.\n",
        "\n",
        "It is not used during training or model tuning.\n",
        "\n",
        "It helps assess generalization ability‚Äîhow well the model can make predictions on new data.\n",
        "\n",
        "üìä Example Workflow:\n",
        "Split your data:\n",
        "\n",
        "80% ‚Üí Training set\n",
        "\n",
        "20% ‚Üí Test set\n",
        "\n",
        "Train the model on the training set.\n",
        "\n",
        "Evaluate the model on the test set using metrics like:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "RMSE, MAE (for regression)\n",
        "\n"
      ],
      "metadata": {
        "id": "00hSstfVnX_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. How do we split data for model fitting (training and testing) in Python?\n",
        "#How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "oDXa2MRZneI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset (e.g., DataFrame with features X and target y)\n",
        "data = pd.DataFrame({\n",
        "    'Age': [22, 25, 47, 52, 46, 56, 48, 55],\n",
        "    'Salary': [15000, 29000, 48000, 60000, 52000, 61000, 58000, 63000],\n",
        "    'Purchased': [0, 0, 1, 1, 1, 1, 1, 1]\n",
        "})\n",
        "\n",
        "X = data[['Age', 'Salary']]   # Features\n",
        "y = data['Purchased']         # Target label\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\\n\", X_train)\n",
        "print(\"Testing set:\\n\", X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b6jJSq1ngUu",
        "outputId": "bfebeeeb-1226-4298-cc4d-d2858751b45a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "    Age  Salary\n",
            "0   22   15000\n",
            "7   55   63000\n",
            "2   47   48000\n",
            "4   46   52000\n",
            "3   52   60000\n",
            "6   48   58000\n",
            "Testing set:\n",
            "    Age  Salary\n",
            "1   25   29000\n",
            "5   56   61000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Parameters:\n",
        "test_size=0.2 ‚Üí 20% data used for testing.\n",
        "\n",
        "random_state=42 ‚Üí Ensures reproducibility (you get the same split every time).\n",
        "\n",
        "You can also stratify the split using stratify=y for balanced class distributions.\n",
        "\n",
        "2. #How Do You Approach a Machine Learning Problem?\n",
        "Approaching an ML problem involves a structured workflow. Here‚Äôs a step-by-step guide:\n",
        "\n",
        "üìä Step-by-Step Machine Learning Workflow:\n",
        "Step\tWhat You Do\n",
        "1. Define the Problem\tIs it classification, regression, clustering, etc.?\n",
        "2. Collect Data\tGather data from files, APIs, or databases.\n",
        "3. Explore and Understand the Data (EDA)\tUse visualizations and statistics to understand patterns and relationships.\n",
        "4. Preprocess Data\tHandle missing values, encode categories, scale/normalize features.\n",
        "5. Split Data\tUse train_test_split to divide into training and testing datasets.\n",
        "6. Choose a Model\tStart with simple models (e.g., Logistic Regression, Decision Tree) depending on the task.\n",
        "7. Train the Model\tFit the model using the training data.\n",
        "8. Evaluate the Model\tUse the test data to evaluate with metrics like accuracy, F1-score, RMSE, etc.\n",
        "9. Tune Hyperparameters (Optional)\tUse cross-validation, grid search, or random search to optimize model performance.\n",
        "10. Finalize and Deploy\tSave the model and integrate it into a production environment or make predictions on new data."
      ],
      "metadata": {
        "id": "_WK3yAUung1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Why do we have to perform EDA before fitting a model to the data?\n"
      ],
      "metadata": {
        "id": "Brks6Me0noXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) is a crucial step in the machine learning workflow. It involves visually and statistically analyzing the dataset before building any model.\n",
        "\n",
        "üìå Reasons to Perform EDA Before Modeling:\n",
        "1. üîç Understand the Data Structure\n",
        "Know the number of features, data types, and distributions.\n",
        "\n",
        "Identify target variable behavior (e.g., balanced or imbalanced classes).\n",
        "\n",
        "Example: Is ‚ÄúSalary‚Äù normally distributed? Are there unexpected data types?\n",
        "\n",
        "2. ‚ùì Detect Missing Values\n",
        "Helps you decide how to handle them (drop, fill, impute).\n",
        "\n",
        "Missing values can break model training if not treated properly.\n",
        "\n",
        "Example: If 20% of rows in \"Age\" are missing, should we fill with mean or drop?\n",
        "\n",
        "3. ‚ö†Ô∏è Identify Outliers\n",
        "Outliers can skew model results, especially in linear models.\n",
        "\n",
        "EDA helps visualize them using box plots, scatter plots, etc.\n",
        "\n",
        "Example: One person's salary is ‚Çπ10 crore while the average is ‚Çπ50,000.\n",
        "\n",
        "4. üéØ Understand Feature Relationships\n",
        "Know which features are strongly correlated with the target.\n",
        "\n",
        "Avoid multicollinearity (strong correlation between features).\n",
        "\n",
        "Example: Are \"Age\" and \"Experience\" highly correlated?\n",
        "\n",
        "5. üß™ Choose Appropriate Preprocessing\n",
        "Based on EDA, you decide:\n",
        "\n",
        "Which features to encode (categorical variables)?\n",
        "\n",
        "Which ones to scale or normalize?\n",
        "\n",
        "Whether to engineer new features?\n",
        "\n",
        "6. üìä Class Imbalance Detection\n",
        "EDA helps identify whether the dataset is imbalanced, especially for classification.\n",
        "\n",
        "If so, you may need to use techniques like SMOTE, undersampling, or class weighting.\n",
        "\n",
        "7. üß† Generate Hypotheses\n",
        "Spot trends or patterns to guide feature selection or transformation.\n",
        "\n",
        "E.g., ‚ÄúYounger users spend more on mobile apps‚Äù ‚Üí create an age group feature.\n",
        "\n"
      ],
      "metadata": {
        "id": "qMX_5eFToXiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.What is correlation?"
      ],
      "metadata": {
        "id": "pVaUHATvoYg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "It tells you whether an increase in one variable leads to an increase or decrease in another.\n",
        "\n",
        "The most common measure is Pearson's correlation coefficient (r).\n",
        "\n",
        "üìà Pearson Correlation Coefficient (r):\n",
        "Value of r\tMeaning\n",
        "+1\tPerfect positive correlation (both increase together)\n",
        "0\tNo correlation (no linear relationship)\n",
        "‚Äì1\tPerfect negative correlation (one increases, other decreases)\n",
        "\n",
        "üîç Formula (Pearson r):\n",
        "ùëü\n",
        "=\n",
        "Cov\n",
        "(\n",
        "ùëã\n",
        ",\n",
        "ùëå\n",
        ")\n",
        "ùúé\n",
        "ùëã\n",
        "‚ãÖ\n",
        "ùúé\n",
        "ùëå\n",
        "r=\n",
        "œÉ\n",
        "X\n",
        "‚Äã\n",
        " ‚ãÖœÉ\n",
        "Y\n",
        "‚Äã\n",
        "\n",
        "Cov(X,Y)\n",
        "‚Äã\n",
        "\n",
        "Cov(X, Y): Covariance between variables X and Y\n",
        "\n",
        "œÉ: Standard deviation"
      ],
      "metadata": {
        "id": "gJ4YH3Ltofys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.What does negative correlation mean?"
      ],
      "metadata": {
        "id": "hIXRn4caogtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "\n",
        "üìâ Key Characteristics:\n",
        "The correlation coefficient (r) is less than 0 (between ‚Äì1 and 0).\n",
        "\n",
        "The variables move in opposite directions.\n",
        "\n",
        "üß† Examples of Negative Correlation:\n",
        "Variable A\tVariable B\tWhat Happens?\n",
        "Study time\tNumber of mistakes on test\tMore studying ‚Üí fewer mistakes\n",
        "Exercise frequency\tBody fat percentage\tMore exercise ‚Üí less body fat\n",
        "Speed of a vehicle\tTravel time\tHigher speed ‚Üí lower travel time\n",
        "\n",
        "üìä Visual Representation:\n",
        "In a scatter plot:\n",
        "\n",
        "As points go from left to right, they trend downward.\n",
        "\n",
        "üìå Degrees of Negative Correlation:\n",
        "Correlation (r)\tInterpretation\n",
        "‚Äì1.0\tPerfect negative correlation\n",
        "‚Äì0.8 to ‚Äì0.6\tStrong negative correlation\n",
        "‚Äì0.4 to ‚Äì0.2\tWeak negative correlation\n",
        "~ 0\tNo linear correlation\n",
        "\n"
      ],
      "metadata": {
        "id": "cH2wZ4cFokgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. How can you find correlation between variables in Python?\n"
      ],
      "metadata": {
        "id": "4UbwooXkor2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can easily calculate the correlation coefficient between variables using Pandas or NumPy.\n",
        "\n",
        "üîπ 1. Using Pandas .corr() Method (Most Common)\n",
        "üß™ Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Age': [21, 25, 30, 35, 40],\n",
        "    'Salary': [25000, 30000, 40000, 50000, 60000],\n",
        "    'Experience': [1, 2, 5, 8, 10]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n",
        "üìå Output (Example):\n",
        "markdown\n",
        "Copy\n",
        "Edit\n",
        "                 Age    Salary  Experience\n",
        "Age         1.000000  0.987654    0.998765\n",
        "Salary      0.987654  1.000000    0.978432\n",
        "Experience  0.998765  0.978432    1.000000\n",
        "1.0 means perfect correlation with itself.\n",
        "\n",
        "Values close to 1 or ‚Äì1 indicate strong positive or negative correlation.\n",
        "\n",
        "Values near 0 indicate weak or no linear correlation.\n",
        "\n",
        "üîπ 2. Using NumPy for Pearson Correlation\n",
        "üß™ Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import numpy as np\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [10, 9, 7, 6, 4]\n",
        "\n",
        "correlation = np.corrcoef(x, y)\n",
        "print(correlation)\n",
        "üìå Output:\n",
        "lua\n",
        "Copy\n",
        "Edit\n",
        "[[ 1.         -0.9819805 ]\n",
        " [-0.9819805   1.        ]]\n",
        "Correlation between x and y is about ‚Äì0.98, indicating strong negative correlation.\n",
        "\n",
        "üîπ 3. Optional: Visualize Correlation with a Heatmap\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "This will display a color-coded matrix showing correlation values ‚Äî great for quickly spotting relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "sFgMOZGqovCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. What is causation? Explain difference between correlation and causation with an example"
      ],
      "metadata": {
        "id": "3XVe3iMoo45R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation means that one event directly causes another. In other words:\n",
        "\n",
        "üîÅ Change in Variable A produces a change in Variable B.\n",
        "\n",
        "So, A ‚Üí B (A causes B).\n",
        "\n",
        "üîÅ Difference Between Correlation and Causation\n",
        "Feature\tCorrelation\tCausation\n",
        "Definition\tA statistical relationship between two variables\tA cause-effect relationship between two variables\n",
        "Direction\tVariables move together (positively or negatively)\tOne variable produces a change in the other\n",
        "Implies Cause?\t‚ùå No\t‚úÖ Yes\n",
        "Can be Coincidental?\t‚úÖ Yes\t‚ùå No\n",
        "Tested with\tCorrelation coefficient (e.g. Pearson's r)\tExperiments, A/B testing, controlled studies\n",
        "\n",
        "üß† Example: Correlation ‚â† Causation\n",
        "Observation:\n",
        "In summer, ice cream sales and drowning incidents both increase.\n",
        "\n",
        "Variable A\tVariable B\tWhat‚Äôs Going On?\n",
        "Ice cream sales ‚Üë\tDrownings ‚Üë\t‚ùå Correlated but NOT causal\n",
        "Actual Cause\tHot weather ‚Üë\t‚úÖ Third variable causes both\n",
        "\n",
        "‚úÖ Interpretation:\n",
        "These two variables are positively correlated, but eating ice cream does not cause drowning.\n",
        "\n",
        "The third factor (hot weather) causes both: people swim more (drowning risk) and eat more ice cream.\n",
        "\n",
        "üìå Key Takeaway:\n",
        "Just because two variables move together doesn‚Äôt mean one causes the other.\n",
        "\n",
        "To prove causation, we need controlled experiments, not just statistical relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "AP9Hr2nuo7mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "5Tnv9u8QpBTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm that adjusts the model's parameters (like weights and biases) to minimize the loss function during training.\n",
        "\n",
        "üß† Goal of an optimizer:\n",
        "Find the set of parameters that gives the lowest possible loss, i.e., the best model performance.\n",
        "\n",
        "üîß Why Do We Need Optimizers?\n",
        "Because the model learns by minimizing a loss function, and the optimizer guides this process using methods like gradient descent.\n",
        "\n",
        "üöÄ Common Types of Optimizers (Especially in Deep Learning)\n",
        "Optimizer\tDescription\tBest For\n",
        "1. Gradient Descent (GD)\tBasic optimizer that updates weights using the entire dataset\tSmall datasets\n",
        "2. Stochastic Gradient Descent (SGD)\tUpdates weights using one sample at a time\tFaster, but noisier\n",
        "3. Mini-Batch Gradient Descent\tCompromise between GD and SGD; updates using small batches\tMost commonly used\n",
        "4. Momentum\tAdds memory of previous steps to smooth updates\tAvoids oscillations\n",
        "5. AdaGrad\tAdaptive learning rate for each parameter\tSparse data, e.g., NLP\n",
        "6. RMSprop\tFixes AdaGrad's learning rate decay issue\tRecurrent Neural Networks (RNNs)\n",
        "7. Adam (Adaptive Moment Estimation)\tCombines Momentum + RMSprop; most popular\tAlmost all deep learning models\n",
        "\n",
        "‚úÖ 1. Gradient Descent (GD)\n",
        "Updates weights using the formula:\n",
        "\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "‚ãÖ\n",
        "‚àá\n",
        "ùêø\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "Œ∏=Œ∏‚àíŒ∑‚ãÖ‚àáL(Œ∏)\n",
        "ùúÉ\n",
        "Œ∏: parameters (weights)\n",
        "\n",
        "ùúÇ\n",
        "Œ∑: learning rate\n",
        "\n",
        "‚àá\n",
        "ùêø\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "‚àáL(Œ∏): gradient of loss with respect to weights\n",
        "\n",
        "üß† Example:\n",
        "If your loss function =\n",
        "(\n",
        "ùë¶\n",
        "‚àí\n",
        "ùë¶\n",
        "^\n",
        ")\n",
        "2\n",
        "(y‚àí\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        " , GD helps minimize this across the full dataset.\n",
        "\n",
        "‚úÖ 2. Stochastic Gradient Descent (SGD)\n",
        "Updates weights after each training sample, which makes it faster but less stable.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='mse')\n",
        "‚úÖ 3. Mini-Batch Gradient Descent\n",
        "Trains on small batches (e.g., 32 samples) instead of full dataset or one sample.\n",
        "\n",
        "Balances speed and accuracy. Common in practice.\n",
        "\n",
        "‚úÖ 4. Momentum Optimizer\n",
        "Adds \"momentum\" to the updates (like physics: inertia).\n",
        "\n",
        "Helps the model avoid local minima and reduce oscillations.\n",
        "\n",
        "‚úÖ 5. AdaGrad\n",
        "Adjusts the learning rate for each parameter.\n",
        "\n",
        "Good for sparse data like text or image pixels.\n",
        "\n",
        "‚úÖ 6. RMSprop\n",
        "Fixes AdaGrad's issue of decaying learning rate.\n",
        "\n",
        "Maintains a moving average of squared gradients.\n",
        "\n",
        "Great for RNNs.\n",
        "\n",
        "‚úÖ 7. Adam (Most Popular)\n",
        "Combines benefits of Momentum and RMSprop.\n",
        "\n",
        "Adapts learning rates for each parameter.\n",
        "\n",
        "Works well in most problems.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')"
      ],
      "metadata": {
        "id": "J5oLBv9ypFC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17. What is sklearn.linear_model ?\n"
      ],
      "metadata": {
        "id": "GxfC-dAzpJSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in the Scikit-learn library that provides a wide range of linear models for regression and classification tasks.\n",
        "\n",
        "It includes implementations of classic algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, and more ‚Äî all built with easy-to-use APIs for training and prediction.\n",
        "\n",
        "üîß Common Models in sklearn.linear_model:\n",
        "Model Name\tPurpose\tUse Case Example\n",
        "LinearRegression\tRegression\tPredicting house prices, salary\n",
        "LogisticRegression\tClassification\tSpam detection, disease prediction\n",
        "Ridge\tRegression with L2 regularization\tWhen you want to prevent overfitting\n",
        "Lasso\tRegression with L1 regularization\tFeature selection + shrinkage\n",
        "ElasticNet\tCombines L1 and L2 penalties\tSparse but stable models\n",
        "SGDClassifier / SGDRegressor\tLinear models trained using stochastic gradient descent\tLarge-scale problems\n",
        "Perceptron\tBinary classification (basic neural model)\tLinearly separable data\n",
        "BayesianRidge\tRegression with Bayesian inference\tProbabilistic regression\n",
        "\n",
        "üß™ Example: Linear Regression\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "print(\"Prediction for 6:\", model.predict([[6]]))\n",
        "üß† Key Benefits of sklearn.linear_model:\n",
        "‚úÖ Simple syntax\n",
        "\n",
        "‚úÖ Optimized performance\n",
        "\n",
        "‚úÖ Includes support for regularization (Ridge, Lasso)\n",
        "\n",
        "‚úÖ Works well with Scikit-learn tools like pipelines, cross-validation, and grid search\n",
        "\n"
      ],
      "metadata": {
        "id": "W3gdG5m-pNPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "uiw8Iq24pT6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.fit() method in Scikit-learn is used to train a machine learning model using your dataset.\n",
        "\n",
        "It learns patterns from the input data (X) and the target labels (y) by adjusting internal parameters (like weights in linear models).\n",
        "\n",
        "üîß Syntax:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model.fit(X, y)\n",
        "üß† What Happens Internally?\n",
        "The model looks at the feature matrix X and the target values y.\n",
        "\n",
        "It applies the optimization algorithm (e.g., gradient descent).\n",
        "\n",
        "It finds the best-fit parameters that minimize the loss function.\n",
        "\n",
        "üìå Required Arguments:\n",
        "Argument\tDescription\n",
        "X\tFeature matrix (2D array or DataFrame), shape: (n_samples, n_features)\n",
        "y\tTarget labels/values, shape: (n_samples,)\n",
        "\n",
        "üîç Example with Linear Regression:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Input features and target\n",
        "X = np.array([[1], [2], [3], [4], [5]])   # Features\n",
        "y = np.array([1, 4, 9, 16, 25])           # Labels\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)  # This line trains the model\n",
        "‚úÖ Optional Arguments (in some models):\n",
        "Some models (like SGDClassifier, DecisionTreeClassifier, etc.) may accept optional arguments like:\n",
        "\n",
        "sample_weight ‚Äì to assign different weights to samples\n",
        "\n",
        "classes ‚Äì for classification models when using incremental learning\n",
        "\n",
        "epochs, batch_size ‚Äì in neural network frameworks like Keras (not Scikit-learn)\n",
        "\n",
        "üß† After .fit():\n",
        "Once the model is trained, you can:\n",
        "\n",
        "Use model.predict(X_test) to make predictions.\n",
        "\n",
        "Access trained parameters like model.coef_, model.intercept_.\n",
        "\n"
      ],
      "metadata": {
        "id": "6qTfYZtypYnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "QHUpowVZpcBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.predict() function is used to make predictions on new or unseen data using a trained model.\n",
        "\n",
        "It takes input features (X_new) and outputs predicted values or labels, based on what the model has learned during model.fit().\n",
        "\n",
        "üîß Syntax:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "predictions = model.predict(X_new)\n",
        "üìå Required Argument:\n",
        "Argument\tDescription\n",
        "X_new\tA 2D array-like structure (e.g., list, NumPy array, or DataFrame) representing the new input data. Shape: (n_samples, n_features)\n",
        "\n",
        "üß™ Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Training data\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict for new data\n",
        "X_new = np.array([[5], [6]])\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(predictions)  # Output: [10. 12.]\n",
        "üß† What Happens Internally?\n",
        "After training, model.predict(X_new):\n",
        "\n",
        "Applies the learned formula/weights to the new data\n",
        "\n",
        "Returns the predicted output (e.g., price, label, probability)\n",
        "\n",
        "‚ö†Ô∏è Important:\n",
        "You must train the model first using model.fit() before calling .predict().\n",
        "\n",
        "The number of features in X_new must match what the model was trained on."
      ],
      "metadata": {
        "id": "jsebslYwpgWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20. What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "rai_oDT1pmYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning and statistics, features (variables) are generally classified into two main types:\n",
        "\n",
        "1. üî¢ Continuous Variables\n",
        "A continuous variable can take on any numerical value within a range. These are typically measurable quantities.\n",
        "\n",
        "üìå Characteristics:\n",
        "Infinite possible values (within limits)\n",
        "\n",
        "Can be decimal or whole numbers\n",
        "\n",
        "Often used in regression problems\n",
        "\n",
        "üß† Examples:\n",
        "Height (e.g., 167.5 cm)\n",
        "\n",
        "Weight (e.g., 68.2 kg)\n",
        "\n",
        "Temperature (e.g., 36.6¬∞C)\n",
        "\n",
        "Price, salary, age\n",
        "\n",
        "2. üî† Categorical Variables\n",
        "A categorical variable represents discrete categories or groups. These are non-numeric labels, or numeric values representing labels.\n",
        "\n",
        "üìå Characteristics:\n",
        "Finite set of values\n",
        "\n",
        "Values represent types, not magnitudes\n",
        "\n",
        "Often used in classification problems\n",
        "\n",
        "üß† Examples:\n",
        "Gender: Male, Female, Other\n",
        "\n",
        "City: Delhi, Mumbai, Kolkata\n",
        "\n",
        "Education: High School, Graduate, Postgraduate\n",
        "\n",
        "Colors: Red, Green, Blue\n",
        "\n",
        "üîÅ Subtypes of Categorical Variables:\n",
        "Type\tDescription\tExample\n",
        "Nominal\tNo natural order or ranking\tColor: Red, Blue\n",
        "Ordinal\tHas an inherent order or ranking\tSize: Small < Medium < Large\n",
        "\n",
        "‚úÖ Summary Table:\n",
        "Feature Type\tValues Example\tNumeric?\tModel Type\n",
        "Continuous\t5.6, 100.0, 43.2\t‚úÖ Yes\tRegression\n",
        "Categorical (Nominal)\tRed, Blue, Green\t‚ùå/‚úÖ (after encoding)\tClassification\n",
        "Categorical (Ordinal)\tLow, Medium, High\t‚úÖ (if encoded)\tClassification"
      ],
      "metadata": {
        "id": "dCuWRYIQpqw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "9ZvqEneEpu2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is a preprocessing technique used to normalize or standardize the range of independent features (input variables) so that they are on a similar scale.\n",
        "\n",
        "Many machine learning algorithms perform better or converge faster when input features are on the same scale.\n",
        "\n",
        "üîß Why Is Feature Scaling Important?\n",
        "Different features may have different units or magnitudes. For example:\n",
        "\n",
        "Feature\tRange\n",
        "Age\t18 to 100\n",
        "Salary\t‚Çπ10,000 to ‚Çπ2,00,000\n",
        "Height (cm)\t150 to 200\n",
        "\n",
        "Without scaling:\n",
        "\n",
        "Algorithms like KNN, SVM, Logistic Regression, Gradient Descent-based models may give more importance to features with larger ranges.\n",
        "\n",
        "It slows convergence or leads to suboptimal results.\n",
        "\n",
        "üß† Example:\n",
        "Suppose you‚Äôre predicting job performance using:\n",
        "\n",
        "Age: 25\n",
        "\n",
        "Salary: ‚Çπ100,000\n",
        "\n",
        "Here, Salary dominates Age due to its scale, even if Age might be equally important.\n",
        "\n",
        "üöÄ Types of Feature Scaling:\n",
        "Technique\tDescription\tRange\tFunction in Python\n",
        "Min-Max Scaling\tScales features to a fixed range (usually 0 to 1)\t[0, 1]\tMinMaxScaler()\n",
        "Standardization\tCenters around mean = 0, std dev = 1\tNo fixed range\tStandardScaler()\n",
        "Robust Scaling\tUses median & IQR, less sensitive to outliers\tVaries\tRobustScaler()\n",
        "Normalization\tScales each sample to unit norm (for vectors)\tUnit vector\tNormalizer()\n",
        "\n",
        "üì¶ Example Using StandardScaler:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[20, 20000],\n",
        "              [30, 50000],\n",
        "              [40, 100000]])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "‚úÖ Helps in:\n",
        "Faster gradient descent convergence\n",
        "\n",
        "Better model accuracy\n",
        "\n",
        "Avoiding bias toward high-magnitude features\n",
        "\n",
        "Improving distance-based algorithms (KNN, SVM)"
      ],
      "metadata": {
        "id": "UXoGlUHYpxRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "jUSOGOHMp4Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python makes feature scaling simple using the sklearn.preprocessing module.\n",
        "\n",
        "üîß Step-by-Step Example\n",
        "Let's say you have the following dataset:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([\n",
        "    [20, 20000],\n",
        "    [30, 50000],\n",
        "    [40, 100000]\n",
        "])\n",
        "Here, the first column (age) and second column (salary) are on very different scales.\n",
        "\n",
        "üöÄ 1. Standardization (using StandardScaler)\n",
        "Transforms features to have mean = 0 and std = 1\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Standardized Data:\\n\", X_scaled)\n",
        "üöÄ 2. Min-Max Scaling (using MinMaxScaler)\n",
        "Transforms values to a range between 0 and 1\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Min-Max Scaled Data:\\n\", X_scaled)\n",
        "üöÄ 3. Robust Scaling (using RobustScaler)\n",
        "Scales using the median and IQR (good for outliers)\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Robust Scaled Data:\\n\", X_scaled)\n",
        "üöÄ 4. Normalization (using Normalizer)\n",
        "Converts rows to unit vectors (mainly for text or distance-based problems)\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = Normalizer()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Normalized Data:\\n\", X_scaled)"
      ],
      "metadata": {
        "id": "Vn7clVkPp7DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23. What is sklearn.preprocessing?\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-5ck9sIqAxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the Scikit-learn library that provides a wide range of tools to prepare or transform data before feeding it into machine learning models.\n",
        "\n",
        "It includes functions for scaling, normalization, encoding categorical variables, handling missing values, and more.\n",
        "\n",
        "üöÄ Why is sklearn.preprocessing Important?\n",
        "Raw data usually needs to be cleaned and standardized for models to learn effectively.\n",
        "This module helps you:\n",
        "\n",
        "Put numerical features on the same scale\n",
        "\n",
        "Convert text labels to numeric form\n",
        "\n",
        "Prepare sparse or categorical data\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "üîß Key Tools in sklearn.preprocessing:\n",
        "Tool / Class\tPurpose\tExample Use\n",
        "StandardScaler\tStandardize data (mean = 0, std = 1)\tSVM, logistic regression, linear models\n",
        "MinMaxScaler\tScale features to a range (e.g. 0‚Äì1)\tNeural networks, distance-based models\n",
        "RobustScaler\tScale using median and IQR (resists outliers)\tDatasets with outliers\n",
        "Normalizer\tScale rows to unit norm (L2 norm = 1)\tText data, cosine similarity\n",
        "OneHotEncoder\tEncode categorical variables as binary arrays\tConvert \"City\" ‚Üí [1,0,0] for \"Delhi\"\n",
        "LabelEncoder\tConvert class labels into integers\tEncode \"Male\" ‚Üí 1, \"Female\" ‚Üí 0\n",
        "Binarizer\tConvert numeric features into binary\tThresholding features (e.g. pass/fail)\n",
        "PolynomialFeatures\tGenerate polynomial & interaction features\tPolynomial regression\n",
        "FunctionTransformer\tApply custom transformations (e.g., log, square root)\tCustom preprocessing\n",
        "\n",
        "üß™ Example: Scaling + Encoding\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Example numeric data\n",
        "X_num = np.array([[10, 200], [15, 300], [20, 400]])\n",
        "\n",
        "# Standardize it\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_num)\n",
        "\n",
        "print(\"Scaled Data:\\n\", X_scaled)"
      ],
      "metadata": {
        "id": "NfX57JcGqEZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "rBr_gwAGqLky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, it‚Äôs important to split your dataset into two parts:\n",
        "\n",
        "Training Set ‚Äì used to train the model\n",
        "\n",
        "Testing Set ‚Äì used to evaluate the model's performance on unseen data\n",
        "\n",
        "üì¶ Tool Used: train_test_split() from sklearn.model_selection\n",
        "üîß Step-by-Step Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample feature data (X) and target labels (y)\n",
        "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
        "y = np.array([10, 20, 30, 40, 50, 60])\n",
        "\n",
        "# Split into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train:\\n\", X_train)\n",
        "print(\"X_test:\\n\", X_test)\n",
        "‚öôÔ∏è Parameters Explained:\n",
        "Parameter\tDescription\n",
        "X\tFeature matrix (input variables)\n",
        "y\tTarget vector (output/label)\n",
        "test_size=0.2\t20% of the data will be used as the test set\n",
        "train_size=0.8\tOptional: 80% will be used as training set\n",
        "random_state=42\tEnsures reproducibility of the split (same result every time)\n",
        "stratify=y\tOptional: Keeps class distribution balanced (useful for classification)\n",
        "\n"
      ],
      "metadata": {
        "id": "Fk24_iFDqOSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25. Explain data encoding?"
      ],
      "metadata": {
        "id": "MDlrecLsqYaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical data (non-numeric) into a numerical format, so that machine learning models can understand and process it.\n",
        "\n",
        "üîç Most machine learning algorithms cannot handle text labels directly ‚Äî they need numbers.\n",
        "\n",
        "üß† Why Is Encoding Needed?\n",
        "Let‚Äôs say you have a column:\n",
        "\n",
        "Color\n",
        "Red\n",
        "Blue\n",
        "Green\n",
        "\n",
        "You can't pass \"Red\" or \"Blue\" directly into a model ‚Äî it must be converted into numbers using encoding techniques.\n",
        "\n",
        "üöÄ Common Types of Encoding:\n",
        "Encoding Type\tDescription\tExample Output\n",
        "Label Encoding\tAssigns a unique number to each category\tRed ‚Üí 0, Blue ‚Üí 1, Green ‚Üí 2\n",
        "One-Hot Encoding\tConverts categories into binary columns\tRed ‚Üí [1,0,0], Blue ‚Üí [0,1,0]\n",
        "Ordinal Encoding\tLike label encoding but for ordered categories\tSmall ‚Üí 1, Medium ‚Üí 2, Large ‚Üí 3\n",
        "Binary Encoding / Target Encoding\tAdvanced encodings for high-cardinality features\te.g., ZIP codes, user IDs\n",
        "\n",
        "‚úÖ 1. Label Encoding\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "colors = ['Red', 'Green', 'Blue', 'Red']\n",
        "encoded = le.fit_transform(colors)\n",
        "print(encoded)  # Output: [2 1 0 2]\n",
        "‚úÖ 2. One-Hot Encoding\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "colors = np.array([['Red'], ['Green'], ['Blue'], ['Red']])\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "one_hot = encoder.fit_transform(colors)\n",
        "print(one_hot)\n",
        "Output:\n",
        "lua\n",
        "Copy\n",
        "Edit\n",
        "[[0. 0. 1.]\n",
        " [0. 1. 0.]\n",
        " [1. 0. 0.]\n",
        " [0. 0. 1.]]\n",
        "Each column represents a color (Blue, Green, Red).\n",
        "\n",
        "‚úÖ 3. Ordinal Encoding\n",
        "Use this when categories have natural order.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Small'], ['Medium'], ['Large']]\n",
        "encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n",
        "print(encoder.fit_transform(data))  # Output: [[0], [1], [2]]\n",
        "‚ö†Ô∏è Important Notes:\n",
        "Use One-Hot Encoding when categories have no natural order.\n",
        "\n",
        "Use Ordinal Encoding when categories are ranked (e.g., education levels).\n",
        "\n",
        "Avoid Label Encoding on nominal features for models like Decision Trees or Logistic Regression ‚Äî it can mislead the model into thinking one category is greater than another."
      ],
      "metadata": {
        "id": "GskGV0hJqbN6"
      }
    }
  ]
}